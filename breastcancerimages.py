# -*- coding: utf-8 -*-
"""BreastCancerImages.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1eOR8nLUTCSp4pzhHDYOjALKimv_-iY_T
"""

#from google.colab import drive
#drive.mount('/content/drive')

#!unzip "/content/drive/MyDrive/archive1.zip" -d "/content/dataset"

#!ls /content/dataset

# üì¶ Installation (already done in your case)
import cv2
import os
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from skimage.feature import local_binary_pattern, graycomatrix, graycoprops  # ‚úÖ updated import
from scipy.stats import skew, kurtosis
from tqdm import tqdm
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import classification_report, confusion_matrix
from sklearn.svm import SVC
from sklearn.metrics import roc_auc_score # Import roc_auc_score
import joblib
#!pip install lightgbm
from lightgbm import LGBMClassifier
from tensorflow import keras
from tensorflow.keras import layers
from tensorflow.keras.callbacks import EarlyStopping
from tensorflow import keras  # Dans le script d'entra√Ænement
from sklearn.metrics import accuracy_score, roc_auc_score, classification_report

def extract_features(img):
    """
    Extrait un ensemble de features radiomics depuis une image d'histopathologie.
    Retourne un dictionnaire de valeurs num√©riques.
    """

    # Convertir en niveau de gris
    gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)
    features = {}

    # ---- üìä Statistiques d'intensit√© ----
    vals = gray.ravel()
    features["mean"] = np.mean(vals)
    features["std"] = np.std(vals)
    features["median"] = np.median(vals)
    features["skew"] = skew(vals)
    features["kurtosis"] = kurtosis(vals)

    # ---- üé® Statistiques de couleur ----
    for i, c in enumerate(["blue", "green", "red"]):
        features[f"{c}_mean"] = np.mean(img[:,:,i])
        features[f"{c}_std"] = np.std(img[:,:,i])

    # ---- üß± Texture GLCM ----
    gray_q = (gray / 16).astype(np.uint8)  # quantisation : 16 niveaux
    glcm = graycomatrix(gray_q, [1], [0], levels=16, symmetric=True, normed=True)

    features["contrast"] = graycoprops(glcm, 'contrast')[0,0]
    features["dissimilarity"] = graycoprops(glcm, 'dissimilarity')[0,0]
    features["homogeneity"] = graycoprops(glcm, 'homogeneity')[0,0]
    features["energy"] = graycoprops(glcm, 'energy')[0,0]
    features["correlation"] = graycoprops(glcm, 'correlation')[0,0]
    features["ASM"] = graycoprops(glcm, 'ASM')[0,0]

    # ---- üîµ Texture LBP ----
    lbp = local_binary_pattern(gray, P=8, R=1, method="uniform")
    hist, _ = np.histogram(lbp.ravel(), bins=np.arange(0, 10), density=True)

    for i in range(9):
        features[f"lbp_{i}"] = hist[i]

    return features

def extract_dataset(root_folder, limit=None):
    """
    Parcourt tous les dossiers du dataset.
    Chaque sous-dossier contient /0 et /1 correspondant aux labels.
    limit = nombre d'images max √† traiter (pour tester rapidement)
    """
    rows = []
    counter = 0

    patients = os.listdir(root_folder)

    for folder in tqdm(patients, desc="Patients"):
        patient_path = os.path.join(root_folder, folder)

        if not os.path.isdir(patient_path):
            continue

        # Dossiers de classe (0 = sain, 1 = cancer)
        for label in ["0", "1"]:
            class_path = os.path.join(patient_path, label)
            if not os.path.isdir(class_path):
                continue

            for filename in os.listdir(class_path):
                img_path = os.path.join(class_path, filename)

                img = cv2.imread(img_path)
                if img is None:
                    continue

                feats = extract_features(img)
                feats["label"] = int(label)
                feats["patient"] = folder
                feats["filename"] = filename

                rows.append(feats)
                counter += 1

                if limit and counter >= limit:
                    return pd.DataFrame(rows)

    return pd.DataFrame(rows)



try:
    df = pd.read_pickle("breast_features.pkl")
    print("‚úîÔ∏è Fichier trouv√©, pas besoin de r√©extraire !")
except:
    print("‚ùå Pas de fichier, extraction en cours...")
    df = extract_dataset(DATASET_PATH, limit=None)
    df.to_pickle("breast_features.pkl")
    print("‚úîÔ∏è Extraction termin√©e et fichier sauvegard√© !")

print(df['label'].value_counts())

#separation
X = df.drop(columns=["label", "patient", "filename"])
y = df["label"]

#train/test split
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42, stratify=y
)

scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

joblib.dump(scaler, "scaler.pkl")
print("‚úîÔ∏è Scaler sauvegard√© sous 'scaler.pkl' !")

"""**LightGBM**"""

#LightGBM
lgbm = LGBMClassifier(
    n_estimators=500,
    learning_rate=0.05,
    max_depth=-1,
    num_leaves=31,
    subsample=0.9,
    colsample_bytree=0.9,
    random_state=42,
    n_jobs=-1,
    class_weight="balanced"
)

lgbm.fit(X_train, y_train)

#evaluation LightGBM
y_pred = lgbm.predict(X_test)
print(classification_report(y_test, y_pred))
print(confusion_matrix(y_test, y_pred))

"""**ANN**"""

# # Architecture ANN optimis√©e
# model = keras.Sequential([
#     layers.Input(shape=(X_train_scaled.shape[1],)),
#     layers.Dense(128, activation='relu'),
#     layers.Dropout(0.3),

#     layers.Dense(64, activation='relu'),
#     layers.Dropout(0.2),

#     layers.Dense(32, activation='relu'),

#     layers.Dense(1, activation='sigmoid')   # Classification binaire
# ])

# model.compile(
#     optimizer=keras.optimizers.Adam(learning_rate=0.0008),
#     loss='binary_crossentropy',
#     metrics=['accuracy', keras.metrics.AUC()]
# )

# model.summary()

# # Early stopping pour √©viter overfitting
# early_stop = EarlyStopping(
#     monitor='val_loss',
#     patience=10,
#     restore_best_weights=True
# )

# # Entra√Ænement
# history = model.fit(
#     X_train_scaled, y_train,
#     validation_split=0.2,
#     epochs=100,
#     batch_size=32,
#     callbacks=[early_stop],
#     verbose=1
# )

# import matplotlib.pyplot as plt

# plt.plot(history.history['loss'], label='Train Loss')
# plt.plot(history.history['val_loss'], label='Val Loss')
# plt.legend()
# plt.title("Loss evolution")
# plt.show()

# plt.plot(history.history['accuracy'], label='Train Acc')
# plt.plot(history.history['val_accuracy'], label='Val Acc')
# plt.legend()
# plt.title("Accuracy evolution")
# plt.show()

# y_pred_prob = model.predict(X_test_scaled).flatten()
# y_pred = (y_pred_prob >= 0.5).astype(int)

# print(classification_report(y_test, y_pred))

# acc = accuracy_score(y_test, y_pred)
# auc = roc_auc_score(y_test, y_pred_prob)

# print(f"Accuracy : {acc:.4f}")
# print(f"AUC : {auc:.4f}")

#AUC Score (obligatoire en m√©decine) avec LightGBM
y_prob = lgbm.predict_proba(X_test)[:, 1]
auc = roc_auc_score(y_test, y_prob)
print("AUC Score :", auc)
y_prob_lgbm = lgbm.predict_proba(X_test_scaled)[:, 1]
auc_lgbm = roc_auc_score(y_test, y_prob_lgbm)

print("AUC LightGBM :", auc_lgbm)

joblib.dump(lgbm, "best_model_lgbm.pkl")
print("‚úîÔ∏è Mod√®le LightGBM sauvegard√© !")


# --------------------------
# 2) ANN (MLP) AUC + save
# # --------------------------

# y_prob_ann = model.predict(X_test_scaled).flatten()
# auc_ann = roc_auc_score(y_test, y_prob_ann)

# print("AUC ANN :", auc_ann)

# # Sauvegarde du mod√®le ANN
# model.save("best_model_ann.h5")
# print("‚úîÔ∏è Mod√®le ANN sauvegard√© !")

# print("üéØ Les deux mod√®les sont pr√™ts pour l‚ÄôAPI m√©dicale.")

# # 6. Sauvegarde des mod√©les
joblib.dump(lgbm, "best_model_lgbm.pkl")
# print("‚úîÔ∏è Mod√®le LightGBM sauvegard√© !")
# model.save("best_model_ann.h5")
# print("‚úîÔ∏è Mod√®le ANN sauvegard√© !")